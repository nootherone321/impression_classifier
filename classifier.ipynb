{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import csv\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import FreqDist, Text\n",
    "from nltk import bigrams, trigrams\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "\n",
    "#from tqdm import tqdm \n",
    "#from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set file output path (current directory), default id\n",
    "id = 0\n",
    "input_path = \"./input/\"\n",
    "output_path = \"./output/\"\n",
    "data_path = input_path + \"resident_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate variables\n",
    "list_addendums = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create pandas dataframes containing ABR guide, resident report data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe & import ABR data taken from 2018 core exam study guide\n",
    "df_abr = pd.DataFrame()\n",
    "df_abr = pd.read_csv(input_path + \"abr_output_items_edited.csv\",usecols=[0,1], dtype=object)\n",
    "df_abr.columns = ['section','item']\n",
    "\n",
    "# create dataframe & import CSV data containing report text\n",
    "df_reports = pd.DataFrame()\n",
    "df_reports = pd.read_csv(input_path + \"montage_bgg_anon.csv\", usecols=[4,8], dtype=object)\n",
    "df_reports.columns = ['modality','report_text']\n",
    "\n",
    "# remove duplicate reports\n",
    "df_reports = df_reports.drop_duplicates(subset='report_text', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function for cleaning each report - IMPORTANT !\n",
    "#     first: convert everything to lowercase\n",
    "\n",
    "    # tag/replace exceptional reports:\n",
    "    # * reports lacking \"FINDINGS\" (i.e. chest section CXR)\n",
    "    # * consult to reference\n",
    "    # * association of exams\n",
    "    # * event report\n",
    "    # -- will ignore for now\n",
    "\n",
    "\n",
    "def extract_impression(text):\n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    if 'findings:' not in text:\n",
    "        if 'This study was initially nominated' in text:\n",
    "            return ['a \\\"consult to reference\\\" report']\n",
    "        elif 'assocation of exams' in text:\n",
    "            return ['association of exams']\n",
    "        elif 'event report' in text:\n",
    "            return ['an event report']\n",
    "        else:\n",
    "            return [\"chest section x-ray report\"]\n",
    "\n",
    "    \n",
    "    # Part 1: carve out the impression chunk\n",
    "    text = text.split('impression:')[1].strip()\n",
    "    \n",
    "    # Remove everything following \"PLAN:\" or \"Dictated by:\" or consult verbiage\n",
    "    if 'plan:' in text:\n",
    "        text = text.split('plan:')[0].rstrip()\n",
    "    elif 'dictated by:' in text:\n",
    "        text = text.split('dictated by:')[0].rstrip()\n",
    "    elif 'electronically signed' in text:\n",
    "        text = text.split('electronically signed')[0].rstrip()\n",
    "    elif 'the findings, conclusions and recommendations' in text:\n",
    "        text = text.split('the findings, conclusions and recommendations')[0].rstrip()\n",
    "\n",
    "    # Handle addendums\n",
    "    if 'addendum' in text:\n",
    "        text_addendum_split = text.split('addendum',maxsplit=1)\n",
    "        text = text_addendum_split.pop(0).rstrip()\n",
    "        list_addendums.append( text_addendum_split[0].rstrip() )\n",
    "\n",
    "          \n",
    "    # remove commas\n",
    "    text = text.replace(',',' ')\n",
    "    \n",
    "    # detect if report was dictated using enumeration\n",
    "    if ('1.' in text[0:3]) or ('1) ' in text[0:3]):\n",
    "        p = re.compile('\\s*\\n*[0-9]{1,2}[\\.\\)]\\s')\n",
    "        text = p.split(text)\n",
    "    else:\n",
    "        p = re.compile('\\.\\n+')\n",
    "        text = p.split(text)\n",
    "    \n",
    "    # Finally:\n",
    "    # filter() method allows a function performed over each element of iterable\n",
    "    #     in this case, 'None' indicates that all empty values are dropped (remove blank lines)\n",
    "    text = list(filter(None, text ))\n",
    "    \n",
    "    return text\n",
    "\n",
    "# function for removing custom stop word list -- currently hard-coded\n",
    "\n",
    "def remove_stop_words(stringlist):\n",
    "    output = []\n",
    "    for s in stringlist:\n",
    "        # split string into words\n",
    "        tokenized = s.split()\n",
    "        set_stop_words = {\"**\",\"::\",\"patient\",\"patient's\",\"successful\",\"placement\", \"findings\",\"mild\",\"moderate\",\"severe\",\"trace\",\"small\",\"large\",\"well\", \"improving\",\"healing\", \"healed\",\"interval\", \"progressive\",\"right\",\"left\",\"french\",\"research\", \"examination\",\"stable\",\"unchanged\", \"redemonstration\", \"redemonstrated\",\"new\",\"aud\",\"i\",\"me\",\"my\",\"myself\",\"we\",\"our\",\"ours\",\"ourselves\",\"you\",\"you're\",\"you've\",\"you'll\",\"you'd\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"he\",\"him\",\"his\",\"himself\",\"she\",\"she's\",\"her\",\"hers\",\"herself\",\"it\",\"it's\",\"its\",\"itself\",\"they\",\"them\",\"their\",\"theirs\",\"themselves\",\"what\",\"which\",\"who\",\"whom\",\"this\",\"that\",\"that'll\",\"these\",\"those\",\"am\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\"being\",\"have\",\"has\",\"had\",\"having\",\"do\",\"does\",\"did\",\"doing\",\"a\",\"an\",\"the\",\"and\",\"but\",\"if\",\"or\",\"because\",\"as\",\"until\",\"while\",\"of\",\"at\",\"by\",\"for\",\"with\",\"about\",\"against\",\"between\",\"into\",\"through\",\"during\",\"before\",\"after\",\"above\",\"below\",\"to\",\"from\",\"up\",\"down\",\"in\",\"out\",\"on\",\"off\",\"over\",\"under\",\"again\",\"further\",\"then\",\"once\",\"here\",\"there\",\"when\",\"where\",\"why\",\"how\",\"all\",\"any\",\"both\",\"each\",\"few\",\"more\",\"most\",\"other\",\"some\",\"such\",\"only\",\"own\",\"same\",\"so\",\"than\",\"too\",\"very\",\"s\",\"t\",\"can\",\"will\",\"just\",\"don\",\"don't\",\"should\",\"should've\",\"now\",\"d\",\"ll\",\"m\",\"o\",\"re\",\"ve\",\"y\",\"ain\",\"aren\",\"aren't\",\"couldn\",\"couldn't\",\"didn\",\"didn't\",\"doesn\",\"doesn't\",\"hadn\",\"hadn't\",\"hasn\",\"hasn't\",\"haven\",\"haven't\",\"isn\",\"isn't\",\"ma\",\"mightn\",\"mightn't\",\"mustn\",\"mustn't\",\"needn\",\"needn't\",\"shan\",\"shan't\",\"shouldn\",\"shouldn't\",\"wasn\",\"wasn't\",\"weren\",\"weren't\",\"won\",\"won't\",\"wouldn\",\"wouldn't\"}\n",
    "        filtered_words = [word for word in tokenized if word.lower() not in set_stop_words]\n",
    "        output.append(' '.join(filtered_words) )\n",
    "    return output\n",
    "\n",
    "def unravel_list(target):\n",
    "    for l in list_imp:\n",
    "        # remove annoying carriage returns \"(\\r\")\n",
    "        #l = l.replace('\\r','')\n",
    "        #l = l.replace('\\n','')\n",
    "        yield l.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean report texts:\n",
    "df_reports['report_text'] = df_reports['report_text'].apply(extract_impression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, our resident data gets granular to the impression-item level.\n",
    "\n",
    "Will take this opportunity to remove additional things:\n",
    "* stray newlines\n",
    "* \"findings communication\" statements\n",
    "* periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_impression_statements = []\n",
    "\n",
    "for entries in df_reports['report_text'].tolist():\n",
    "    for i in entries:\n",
    "        if 'discussed' in i:\n",
    "            i = i.split('discussed')[0]\n",
    "        if 'communicate' in i:\n",
    "            i = i.split('communicate')[0]\n",
    "        i = i.replace('.','')\n",
    "        i = i.replace('\\n',' ').strip()\n",
    "        list_impression_statements.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving to CSV file\n",
    "\n",
    "# report impression block --> list of impression-item strings \n",
    "df_reports.to_csv(output_path+\"initial_impressions.csv\",encoding=\"utf-8\")\n",
    "\n",
    "# addendum items --> individual rows\n",
    "with open(output_path+'addendums.csv', 'w',encoding='utf-8',newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for i in list_addendums:\n",
    "        writer.writerow([i])\n",
    "f.close()\n",
    "\n",
    "# impression item string --> individual rows\n",
    "with open(output_path+'split_impressions.csv', 'w',encoding='utf-8',newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for i in list_impression_statements:\n",
    "        writer.writerow([i])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK eval\n",
    "\n",
    "Will manually create set of \"stop\" words to help out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_stop_words = {\"(\",\")\",\"**\",\"::\",\":\",\"patient\",\"patient's\",\"successful\",\"placement\", \"finding\",\"findings\",\"mild\",\"moderate\",\"severe\",\"trace\",\"gross\",\"small\",\"large\",\"well\", \"improving\",\"healing\", \"healed\",\"interval\", \"progressive\",\"right\",\"left\",\"bilateral\",\"lesion\",\"represent\",\"consistent\",\"may\",\"including\",\"french\",\"research\", \"examination\",\"stable\",\"unchanged\", \"redemonstration\", \"redemonstrated\",\"likely\",\"new\",\"aud\",\"i\",\"me\",\"my\",\"myself\",\"we\",\"our\",\"ours\",\"ourselves\",\"you\",\"you're\",\"you've\",\"you'll\",\"you'd\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"he\",\"him\",\"his\",\"himself\",\"she\",\"she's\",\"her\",\"hers\",\"herself\",\"it\",\"it's\",\"its\",\"itself\",\"they\",\"them\",\"their\",\"theirs\",\"themselves\",\"what\",\"which\",\"who\",\"whom\",\"this\",\"that\",\"that'll\",\"these\",\"those\",\"am\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\"being\",\"have\",\"has\",\"had\",\"having\",\"do\",\"does\",\"did\",\"doing\",\"a\",\"an\",\"the\",\"and\",\"but\",\"if\",\"or\",\"because\",\"as\",\"until\",\"while\",\"of\",\"at\",\"by\",\"for\",\"with\",\"about\",\"against\",\"between\",\"into\",\"through\",\"during\",\"before\",\"after\",\"above\",\"below\",\"to\",\"from\",\"up\",\"down\",\"in\",\"out\",\"on\",\"off\",\"over\",\"under\",\"again\",\"further\",\"then\",\"once\",\"here\",\"there\",\"when\",\"where\",\"why\",\"how\",\"all\",\"any\",\"both\",\"each\",\"few\",\"more\",\"most\",\"other\",\"some\",\"such\",\"only\",\"own\",\"same\",\"so\",\"than\",\"too\",\"very\",\"s\",\"t\",\"can\",\"will\",\"just\",\"don\",\"don't\",\"should\",\"should've\",\"now\",\"d\",\"ll\",\"m\",\"o\",\"re\",\"ve\",\"y\",\"ain\",\"aren\",\"aren't\",\"couldn\",\"couldn't\",\"didn\",\"didn't\",\"doesn\",\"doesn't\",\"hadn\",\"hadn't\",\"hasn\",\"hasn't\",\"haven\",\"haven't\",\"isn\",\"isn't\",\"ma\",\"mightn\",\"mightn't\",\"mustn\",\"mustn't\",\"needn\",\"needn't\",\"shan\",\"shan't\",\"shouldn\",\"shouldn't\",\"wasn\",\"wasn't\",\"weren\",\"weren't\",\"won\",\"won't\",\"wouldn\",\"wouldn't\",\"techniques\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the ABR exam guide data\n",
    "* much here that isn't useful\n",
    "* removed sections\n",
    "    * physics\n",
    "    * RISE\n",
    "    * nuc med "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_abr['item'] = df_abr['item'].str.strip()\n",
    "list_abr_items = df_abr['item'].tolist()\n",
    "\n",
    "# convert list into a single string\n",
    "string_abr_items = ' '.join(list_abr_items)\n",
    "\n",
    "# tokenize words\n",
    "words = word_tokenize(string_abr_items)\n",
    "\n",
    "fdist1 = FreqDist(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('and', 377),\n",
       " ('(', 200),\n",
       " (')', 198),\n",
       " ('of', 146),\n",
       " ('disease', 115),\n",
       " ('Normal', 79),\n",
       " ('syndrome', 78),\n",
       " ('including', 76),\n",
       " ('tumors', 67),\n",
       " ('Congenital', 64)]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist1.most_common(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take existing data and remove Remove \"stop\" words\n",
    "list_abr_item_no_stop = []\n",
    "for w in words:\n",
    "    if w.lower() not in set_stop_words:\n",
    "        list_abr_item_no_stop.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist2 = FreqDist(list_abr_item_no_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('disease', 115),\n",
       " ('Normal', 79),\n",
       " ('syndrome', 78),\n",
       " ('tumors', 67),\n",
       " ('Congenital', 64),\n",
       " ('cell', 45),\n",
       " ('cyst', 41),\n",
       " ('masses', 40),\n",
       " ('Benign', 38),\n",
       " ('Trauma', 38),\n",
       " ('anomalies', 35),\n",
       " ('tumor', 33),\n",
       " ('sign', 33),\n",
       " ('Lymphoma', 32),\n",
       " ('Malignant', 29),\n",
       " ('diseases', 28),\n",
       " ('venous', 27),\n",
       " ('Vascular', 27),\n",
       " ('cysts', 27),\n",
       " ('carcinoma', 26)]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist2.most_common(n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the resident data. First some basics..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert list-impression-items into a single string\n",
    "string_reports = ' '.join(list_impression_statements)\n",
    "# tokenize words\n",
    "words = word_tokenize(string_reports)\n",
    "\n",
    "fdist3 = FreqDist(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('of', 15812),\n",
       " ('the', 12935),\n",
       " ('no', 7369),\n",
       " ('and', 6950),\n",
       " ('with', 5701),\n",
       " ('right', 5608),\n",
       " ('in', 5408),\n",
       " ('left', 5359),\n",
       " ('to', 3854),\n",
       " ('a', 3608)]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist3.most_common(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok. Not terribly informative - a lot of meaningless words here.\n",
    "Will try some cleanup:\n",
    "* remove stop words\n",
    "* remove previously-tagged \"difficult\" impressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take existing data and remove Remove \"stop\" words\n",
    "def is_difficult_item(i):\n",
    "    if 'chest section x-ray' in i:\n",
    "        return True\n",
    "    elif 'association of exams' in i:\n",
    "        return True\n",
    "    elif 'consult to reference' in i:\n",
    "        return True\n",
    "    elif 'an event report' in i:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "        \n",
    "list_impression_statements_no_stop = []\n",
    "for i in list_impression_statements:\n",
    "    if is_difficult_item(i):\n",
    "        continue\n",
    "    tokenized = i.split()\n",
    "    filtered_words = [word for word in tokenized if word.lower() not in set_stop_words]\n",
    "    list_impression_statements_no_stop.append(' '.join(filtered_words) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert list-impression-items into a single string\n",
    "string_reports_no_stop = ' '.join( list_impression_statements_no_stop )\n",
    "# tokenize words\n",
    "words = word_tokenize(string_reports_no_stop)\n",
    "\n",
    "fdist4 = FreqDist(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('no', 7369),\n",
       " ('fracture', 2682),\n",
       " ('evidence', 2337),\n",
       " ('acute', 2253),\n",
       " ('change', 1735),\n",
       " ('normal', 1533),\n",
       " ('disease', 1530),\n",
       " ('pulmonary', 1392),\n",
       " ('lobe', 1017),\n",
       " ('bowel', 954),\n",
       " ('tube', 945),\n",
       " ('lower', 945),\n",
       " ('distal', 931),\n",
       " ('spine', 918),\n",
       " (':', 869),\n",
       " ('recommend', 863),\n",
       " ('cm', 854),\n",
       " ('upper', 811),\n",
       " ('chest', 795),\n",
       " ('within', 793)]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist4.most_common(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ca', \"n't\"), ('james', 'berger'), ('architectural', 'distortion'), ('gamma', 'probe'), ('ejection', 'fraction'), ('the', 'american'), ('referring', 'clinician'), ('subacromial', 'spur'), ('society', 'recommends'), ('qualitative', 'lugano'), ('paracolic', 'gutter'), ('pes', 'planus'), ('standard', 'deviations'), ('dental', 'caries'), ('laryngeal', 'penetration'), ('mucus', 'plugging'), ('likelihood', 'ratio'), ('****', 'acr'), ('submitted', 'hematopathology'), ('appendicular', 'skeleton'), ('extraprostatic', 'spread'), ('tree', 'bud'), ('utilizing', 'digital'), ('tibiofibular', 'syndesmosis'), ('hallux', 'valgus'), ('attending', 'observations'), ('parathyroid', 'adenoma'), ('er', 'physician'), ('necrotizing', 'enterocolitis'), ('kink', 'discontinuity')]\n"
     ]
    }
   ],
   "source": [
    "finder = BigramCollocationFinder.from_words(words)\n",
    "\n",
    "finder.apply_freq_filter(10)\n",
    "\n",
    "print(finder.nbest(bigram_measures.pmi,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('nontunneled', 'dual', 'lumen'), ('dual', 'lumen', 'hohn'), ('cardiac', 'blood', 'pool'), ('differential', 'diagnosis', 'includes'), ('data', 'set', 'thus'), ('set', 'thus', 'contain'), ('source', 'data', 'set'), ('native', 'source', 'data'), ('meet', 'ata', 'guidelines'), ('rotator', 'cuff', 'tear'), ('us', '12-24', 'months'), ('replace', 'initial', 'conclusions'), ('leftward', 'midline', 'shift'), ('accuracy', 'second-opinion', 'interpretation'), ('reduced', 'internally', 'fixated'), ('compartment', 'predominant', 'tricompartmental'), ('recommendations', 'made', 'facility'), ('provided', 'teleradiologist', 'dr'), ('12-24', 'months', 'low'), ('performed', 'based', 'upon'), ('suspicion', '>', '=1'), ('correlation', 'point', 'tenderness'), ('swallowing', 'mechanism', 'abnormal'), ('condition', 'time', 'comparison'), ('please', 'see', 'separate'), ('necessary', 'provided', 'images'), ('superior', 'vena', 'cava'), ('speech', 'pathology', 'note'), ('history', 'necessary', 'provided'), ('follow-up', 'us', '12-24')]\n"
     ]
    }
   ],
   "source": [
    "finder2 = TrigramCollocationFinder.from_words(words)\n",
    "\n",
    "finder2.apply_freq_filter(20)\n",
    "\n",
    "print(finder2.nbest(trigram_measures.pmi,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'list_articles' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-106-9121a8d3e016>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# for each term, split it into words (could be just one word) and stem each word\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mstemmed_terms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist_articles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# add 'match anything after it' expression to each of the stemmed words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'list_articles' is not defined"
     ]
    }
   ],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "# for each term, split it into words (could be just one word) and stem each word\n",
    "stemmed_terms = ( (stemmer.stem(word) for word in str(s).split() ) for s in list_articles)\n",
    "\n",
    "# add 'match anything after it' expression to each of the stemmed words\n",
    "# join result into a pattern string\n",
    "regex_patterns = [''.join( clean_articles(stem) + '.*' for stem in term) for term in stemmed_terms]\n",
    "\n",
    "print(\"regex patterns acquired\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in list_impressions:\n",
    "    match_obs = ( re.search(pattern, str(sentence), flags=re.IGNORECASE) for pattern in regex_patterns)\n",
    "    matches = [m.group(0) for m in match_obs if m]\n",
    "    #print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in list_impressions:\n",
    "    # regex_patterns maps directly onto terms (strictly speaking it's one-to-one and onto)\n",
    "    for term, pattern in zip(list_articles, regex_patterns):\n",
    "        if re.search(pattern, str(sentence), flags=re.IGNORECASE):\n",
    "            # process term (put it in the db)\n",
    "            print('TERM: {0} FOUND IN: {1}'.format(term, sentence))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
